{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER = 'P'\n",
    "SPACE  = '.'\n",
    "WALL = '#'\n",
    "TRAP = \"X\"\n",
    "GOAL = \"G\"\n",
    "\n",
    "DOWN = \"down\"\n",
    "LEFT = \"left\"\n",
    "RIGHT = \"right\"\n",
    "UP = \"up\"\n",
    "\n",
    "ACTIONS = [DOWN,LEFT,RIGHT,UP]\n",
    "\n",
    "class Game:\n",
    "    def __init__(self,filePath):\n",
    "        \"\"\"\n",
    "        Assumes a txt file at `filePath` that is structured like this:\\n\n",
    "        5 6 \\n\n",
    "        ##### \\n\n",
    "        #...# \\n\n",
    "        #.XG# \\n\n",
    "        P..C# \\n\n",
    "        #...# \\n\n",
    "        ##### \\n\n",
    "\n",
    "        Where the first line contains 2 numbers that represent the width and height of the grid.\n",
    "\n",
    "        \\# = Wall \\n\n",
    "        . = Open Space \\n\n",
    "        P = Player \\n\n",
    "        F = GOAL position \\n\n",
    "        X = Trap \\n\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.map = []\n",
    "        f = open(filePath, \"r\")\n",
    "        self.score = 0\n",
    "        self.finished = False\n",
    "        for index,line in enumerate(f.readlines()):\n",
    "            if(index == 0):\n",
    "                sizes = line.split(\" \")\n",
    "                self.width = int(sizes[0])\n",
    "                self.height = int(sizes[1])\n",
    "            else:\n",
    "                row = []\n",
    "                for i in range(self.width):\n",
    "                    character = line[i]\n",
    "\n",
    "                    if character == PLAYER:\n",
    "                        row.append(SPACE)\n",
    "                        self.start_position = {\n",
    "                            \"x\": i,\n",
    "                            \"y\": index - 1\n",
    "                        }\n",
    "                        self.player = {\n",
    "                            \"x\": i,\n",
    "                            \"y\": index - 1\n",
    "                        }\n",
    "                    else:\n",
    "                        row.append(character)\n",
    "                self.map.append(row)\n",
    "\n",
    "    def execute(self,action):\n",
    "        \"\"\"Executes the `action` provided. Every action that gets taken `self.score` gets subtracted by 1\"\"\"\n",
    "        px = self.player[\"x\"]\n",
    "        py = self.player[\"y\"]\n",
    "\n",
    "        if(action == \"up\"):\n",
    "            if(py != 0 and self.map[py - 1][px] != WALL):\n",
    "                self.player[\"y\"] -= 1\n",
    "        elif(action == \"down\"):\n",
    "            if(py != self.height - 1 and self.map[py + 1][px] != WALL):\n",
    "                self.player[\"y\"] += 1\n",
    "        elif(action == \"left\"):\n",
    "            if(px != 0 and self.map[py][px - 1] != WALL):\n",
    "                self.player[\"x\"] -= 1\n",
    "        elif(action == \"right\"):\n",
    "            if(px != self.width - 1 and self.map[py][px + 1] != WALL):\n",
    "                self.player[\"x\"] += 1\n",
    "        \n",
    "        if self.isPlayerOnTrap():\n",
    "            self.score -= 100\n",
    "            # self.finished = True\n",
    "            self.player[\"y\"] = self.start_position[\"y\"]\n",
    "            self.player[\"x\"] = self.start_position[\"x\"]\n",
    "            return -100\n",
    "\n",
    "        if self.isPlayerOnGOAL():\n",
    "            self.finished = True\n",
    "            return 0\n",
    "        self.score -= 1\n",
    "        return -1\n",
    "     \n",
    "    def isPlayerOnTrap(self):\n",
    "        \"\"\"Returns `True` if the player is currently on a trap\"\"\"\n",
    "        if self.map[self.player[\"y\"]][self.player[\"x\"]] == TRAP:\n",
    "            return True\n",
    "\n",
    "    def isPlayerOnGOAL(self):\n",
    "        \"\"\"Returns `True` if the player is currently on a GOAL\"\"\"\n",
    "        if self.map[self.player[\"y\"]][self.player[\"x\"]] == GOAL:\n",
    "            return True\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "class algorithm:\n",
    "    def __init__(self, map_name, discount_factor=1, learning_rate=0.5, explore_rate=0.1):\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.explore_rate = explore_rate\n",
    "\n",
    "        self.map_name = map_name\n",
    "        self.game = Game(map_name)\n",
    "        \n",
    "        with open(map_name, \"r\") as f:\n",
    "            sizes = f.readline().split(\" \")\n",
    "            self.width = int(sizes[0])\n",
    "            self.height = int(sizes[1])\n",
    "        \n",
    "        self.state = self.game.player[\"x\"]+self.game.player[\"y\"]*self.width\n",
    "\n",
    "        self.q = np.zeros((self.width*self.height, len(ACTIONS)), dtype=np.float16)\n",
    "\n",
    "    def __call__(self, algorithm, selection_method=\"e_greedy\", episodes=20):\n",
    "        \"\"\"\n",
    "        Play the game 'episodes' times and update the q values using the algorithm 'algorithm'.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        if algorithm == \"sarsa\":\n",
    "            for _ in range(episodes):\n",
    "                # Choose action\n",
    "                action_index = self.action_selection(self.state, selection_method)\n",
    "                action = ACTIONS[action_index]\n",
    "\n",
    "                while not self.game.finished:\n",
    "                    # Execute action and get reward and new state\n",
    "                    reward = self.game.execute(action)\n",
    "                    new_state = self.game.player[\"x\"]+self.game.player[\"y\"]*self.width\n",
    "\n",
    "                    # Choose new action index\n",
    "                    new_action_index = self.action_selection(new_state, selection_method)\n",
    "\n",
    "                    # Calculate variables needed for updating the q value\n",
    "                    dmefr = self.discount_factor*self.q[new_state][new_action_index]\n",
    "                    current_q = self.q[self.state][action_index]\n",
    "\n",
    "                    # Update q value\n",
    "                    self.q[self.state][action_index] += self.learning_rate*(reward+dmefr-current_q)\n",
    "\n",
    "                    # Update state and action\n",
    "                    self.state = new_state\n",
    "                    action_index = new_action_index\n",
    "                    action = ACTIONS[action_index]\n",
    "\n",
    "\n",
    "                scores.append(self.game.score)\n",
    "\n",
    "                # Reset game\n",
    "                self.reset()\n",
    "\n",
    "            # Return the q matrix\n",
    "            return self.q, scores\n",
    "\n",
    "        elif algorithm == \"q_learning\":\n",
    "            for _ in range(episodes):\n",
    "                while not self.game.finished:\n",
    "                    # print(self.state)\n",
    "                    # Choose action\n",
    "                    action_index = self.action_selection(self.state, selection_method)\n",
    "                    action = ACTIONS[action_index]\n",
    "\n",
    "                    # Execute action and get reward and new state\n",
    "                    reward = self.game.execute(action)\n",
    "                    new_state = self.game.player[\"x\"]+self.game.player[\"y\"]*self.width\n",
    "\n",
    "                    # Calculate variables needed for updating the q value\n",
    "                    dmefr = self.discount_factor*max(self.q[new_state])\n",
    "                    current_q = self.q[self.state][action_index]\n",
    "\n",
    "                    # print(reward+dmefr)\n",
    "                    # Update q value\n",
    "                    self.q[self.state][action_index] += self.learning_rate*(reward+dmefr-current_q)\n",
    "\n",
    "                    # Update state\n",
    "                    self.state = new_state\n",
    "\n",
    "                scores.append(self.game.score)\n",
    "\n",
    "                # Reset game\n",
    "                self.reset()\n",
    "\n",
    "            # Return the q matrix\n",
    "            return self.q, scores\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the game\"\"\"\n",
    "        self.game = Game(self.map_name)\n",
    "        self.state = self.game.player[\"x\"]+self.game.player[\"y\"]*self.width\n",
    "\n",
    "    def action_selection(self, state, selection_method):\n",
    "        \"\"\"\n",
    "        Returns the index of a action.\n",
    "        This is a random action if a random integer between 0 and 1 is smaller then the explore rate.\n",
    "        If this is not the case return the action with the highest q value.\n",
    "        \"\"\"\n",
    "        if selection_method == \"e_greedy\":\n",
    "            # If a random number is within the explore rate we explore by choosing a random action\n",
    "            if np.random.uniform(0,1) <= self.explore_rate:\n",
    "                return np.random.choice(range(len(ACTIONS)))\n",
    "            \n",
    "            # If we don't explore we choose the action with the highest reward \n",
    "            # If there are multiple actions with the same reward we choose one of them randomly\n",
    "            max = np.max(self.q[state])\n",
    "            indexes = [i for i, x in enumerate(self.q[state]) if x == max]\n",
    "            return np.random.choice(indexes)\n",
    "\n",
    "        elif selection_method == \"softmax\":\n",
    "            # Choose an action based on the softmax function\n",
    "            return np.random.choice(range(len(ACTIONS)), p=softmax(self.q[state]))\n",
    "\n",
    "        elif selection_method == \"greedy\":\n",
    "            # Choose the action with the highest reward if there are multiple actions \n",
    "            # with the same reward we choose one of them randomly\n",
    "            max = np.max(self.q[state])\n",
    "            indexes = [i for i, x in enumerate(self.q[state]) if x == max]\n",
    "            return np.random.choice(indexes)\n",
    "        \n",
    "        raise ValueError(\"Selection method not found\")\n",
    "\n",
    "\n",
    "def compare_performance():\n",
    "    map = \"./map.txt\"\n",
    "    runs = 1000\n",
    "    episodes = 500\n",
    "    selection_methods = [\"e_greedy\", \"softmax\", \"greedy\"][:1]\n",
    "\n",
    "    for selection_method in selection_methods:\n",
    "\n",
    "        # Initialize the running averages for the rewards at 0\n",
    "        avr_reward_q_learning = [0 for _ in range(episodes)]\n",
    "        avr_reward_sarsa = [0 for _ in range(episodes)]\n",
    "\n",
    "        # Run the algorithms 'runs' times to even out the randomness\n",
    "        for _ in range(runs):\n",
    "            # Initialize the algorithms\n",
    "            q_learning_algo = algorithm(map)\n",
    "            sarsa_algo = algorithm(map)\n",
    "            \n",
    "            # Run the algorithms\n",
    "            q, scores_q_learning = q_learning_algo(\"q_learning\", selection_method=selection_method, episodes=episodes)\n",
    "            q, scores_sarsa = sarsa_algo(\"sarsa\", selection_method=selection_method, episodes=episodes)\n",
    "            \n",
    "            # Add the rewards to the running averages\n",
    "            avr_reward_q_learning = [x+(y/runs) for x,y in zip(avr_reward_q_learning, scores_q_learning)]\n",
    "            avr_reward_sarsa = [x+(y/runs) for x,y in zip(avr_reward_sarsa, scores_sarsa)]\n",
    "\n",
    "        # Add the results of the algorithms for the current selection method to the plot\n",
    "        plt.plot(avr_reward_q_learning, label=\"Q-learning \"+selection_method)\n",
    "        plt.plot(avr_reward_sarsa, label=\"Sarsa \"+selection_method)\n",
    "\n",
    "    # Plot the results of the algorithms\n",
    "    plt.ylim([-100, 0])\n",
    "    plt.legend(fontsize=14)\n",
    "    # plt.title(\"Comparison of Q-learning and Sarsa with different selection methods\")\n",
    "    plt.title(\"Q-learning vs Sarsa using e-greedy selection\", fontsize=14)\n",
    "    plt.xlabel(\"Episodes\", fontsize=14)\n",
    "    plt.ylabel(\"Average reward\", fontsize=14)\n",
    "\n",
    "\n",
    "def compare_policy():\n",
    "    \"\"\"\n",
    "    Compares the policies of the algorithms by printing calculating the q table by running the algorithm \n",
    "    and then for each state printing the action with the highest q value.\n",
    "    \"\"\"\n",
    "    map = \"./map.txt\"\n",
    "    arrows = [\"↓\", \"←\", \"→\", \"↑\"]\n",
    "    episodes = 500\n",
    "    algorithms = [\"q_learning\", \"sarsa\"]\n",
    "\n",
    "    for algo_name in algorithms:\n",
    "\n",
    "        selection_methods = [\"e_greedy\", \"softmax\", \"greedy\"]\n",
    "\n",
    "        for selection_method in selection_methods:\n",
    "\n",
    "            algo = algorithm(map)\n",
    "            q, scores = algo(algo_name, selection_method, episodes)\n",
    "            print()\n",
    "            print(algo_name, \" \", selection_method)\n",
    "            for i in range(algo.game.height):\n",
    "                for j in range(algo.game.width):\n",
    "                    print(arrows[np.argmax(q[i*algo.width + j])], end=\" \")\n",
    "                print()\n",
    "\n",
    "# compare_performance()\n",
    "compare_policy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cb072fc437d34ad4e25b06c62a5166813f133afdb66d14eac81f161a756ec6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
